{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.distributed as dist\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from RNN_model import RNN_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_dictionary = np.load('/Users/liuchunlei/Desktop/IMDB Movie reviews/preprocessed_data/imdb_dictionary.npy')\n",
    "vocab_size = 8000 # imdb_dictionary.shape[0], 8000 can reduce the number of weights without igonoring too much unique tokens\n",
    "\n",
    "x_train = []\n",
    "with open('/Users/liuchunlei/Desktop/IMDB Movie reviews/preprocessed_data/imdb_train.txt','r',encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    line = line.split(' ')\n",
    "    line = np.asarray(line,dtype=np.int)\n",
    "\n",
    "    line[line>vocab_size] = 0\n",
    "\n",
    "    x_train.append(line)\n",
    "x_train = x_train[0:25000]\n",
    "y_train = np.zeros((25000,))\n",
    "y_train[0:12500] = 1 #the first 12500 are positive reviews, the next 12500 are negative reviews, 50000 are unlabelled reviews\n",
    "\n",
    "x_test = []\n",
    "with open('/Users/liuchunlei/Desktop/IMDB Movie reviews/preprocessed_data/imdb_test.txt','r',encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    line = line.split(' ')\n",
    "    line = np.asarray(line,dtype=np.int)\n",
    "\n",
    "    line[line>vocab_size] = 0\n",
    "\n",
    "    x_test.append(line)\n",
    "y_test = np.zeros((25000,))\n",
    "y_test[0:12500] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size += 1\n",
    "batch_size = 200\n",
    "no_of_epochs = 6\n",
    "model = RNN_model(vocab_size,300)\n",
    "# opt = 'sgd'\n",
    "# LR = 0.01\n",
    "opt = 'adam'\n",
    "LR = 0.001\n",
    "if(opt=='adam'):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "elif(opt=='sgd'):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9)\n",
    "L_Y_train = len(y_train) #25000\n",
    "L_Y_test = len(y_test)\n",
    "\n",
    "\n",
    "train_loss = []\n",
    "train_accu = []\n",
    "test_accu = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:43: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 65.47 0.6170 345.4499\n",
      "1 78.18 0.4584 357.8841\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(no_of_epochs):\n",
    "\n",
    "    # training\n",
    "    model.train()\n",
    "\n",
    "    epoch_acc = 0.0\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    epoch_counter = 0\n",
    "\n",
    "    time1 = time.time()\n",
    "    \n",
    "    I_permutation = np.random.permutation(L_Y_train)\n",
    "\n",
    "    for i in range(0, L_Y_train, batch_size):\n",
    "\n",
    "        x_input2 = [x_train[j] for j in I_permutation[i:i+batch_size]] #batchsize by different sequence length\n",
    "        sequence_length = 100\n",
    "        x_input = np.zeros((batch_size,sequence_length),dtype=np.int) #batchsize by same sequence lenght 100\n",
    "        for j in range(batch_size):\n",
    "            x = np.asarray(x_input2[j])\n",
    "            sl = x.shape[0]\n",
    "            if(sl < sequence_length):\n",
    "                x_input[j,0:sl] = x\n",
    "            else:\n",
    "                start_index = np.random.randint(sl-sequence_length+1)\n",
    "                x_input[j,:] = x[start_index:(start_index+sequence_length)]\n",
    "        y_input = y_train[I_permutation[i:i+batch_size]]\n",
    "#         x_input = Variable(torch.FloatTensor(x_input)).cuda()\n",
    "#         target = Variable(torch.FloatTensor(y_input)).cuda()\n",
    "        data = torch.LongTensor(x_input)\n",
    "        target = torch.FloatTensor(y_input)\n",
    "        optimizer.zero_grad()\n",
    "        loss, pred = model(data,target,train=True)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()   # update weights\n",
    "        \n",
    "        prediction = pred >= 0.0\n",
    "        truth = target >= 0.5\n",
    "        acc = prediction.eq(truth).sum().cpu().data.numpy()\n",
    "        epoch_acc += acc\n",
    "        epoch_loss += loss.data[0]\n",
    "        epoch_counter += batch_size\n",
    "\n",
    "    epoch_acc /= epoch_counter\n",
    "    epoch_loss /= (epoch_counter/batch_size)\n",
    "\n",
    "    train_loss.append(epoch_loss)\n",
    "    train_accu.append(epoch_acc)\n",
    "\n",
    "    print(epoch, \"%.2f\" % (epoch_acc*100.0), \"%.4f\" % epoch_loss, \"%.4f\" % float(time.time()-time1))\n",
    "    \n",
    "    if((epoch+1)%3)==0:\n",
    "        # do testing loop\n",
    "\n",
    "        # ## test\n",
    "        model.eval()\n",
    "\n",
    "        epoch_acc = 0.0\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        epoch_counter = 0\n",
    "\n",
    "        time1 = time.time()\n",
    "\n",
    "        I_permutation = np.random.permutation(L_Y_test)\n",
    "\n",
    "        for i in range(0, L_Y_test, batch_size):\n",
    "\n",
    "            x_input2 = [x_train[j] for j in I_permutation[i:i+batch_size]] #batchsize by different sequence length\n",
    "            sequence_length = 100\n",
    "            x_input = np.zeros((batch_size,sequence_length),dtype=np.int) #batchsize by same sequence lenght 100\n",
    "            for j in range(batch_size):\n",
    "                x = np.asarray(x_input2[j])\n",
    "                sl = x.shape[0]\n",
    "                if(sl < sequence_length):\n",
    "                    x_input[j,0:sl] = x\n",
    "                else:\n",
    "                    start_index = np.random.randint(sl-sequence_length+1)\n",
    "                    x_input[j,:] = x[start_index:(start_index+sequence_length)]\n",
    "            y_input = y_train[I_permutation[i:i+batch_size]]\n",
    "            data = torch.LongTensor(x_input)\n",
    "            target = torch.FloatTensor(y_input)\n",
    "            loss, pred = model(data,target,train=True)\n",
    "\n",
    "            prediction = pred >= 0.0\n",
    "            truth = target >= 0.5\n",
    "            acc = prediction.eq(truth).sum().cpu().data.numpy()\n",
    "            epoch_acc += acc\n",
    "            epoch_loss += loss.data[0]\n",
    "            epoch_counter += batch_size\n",
    "\n",
    "        epoch_acc /= epoch_counter\n",
    "        epoch_loss /= (epoch_counter/batch_size)\n",
    "\n",
    "        test_accu.append(epoch_acc)\n",
    "\n",
    "        time2 = time.time()\n",
    "        time_elapsed = time2 - time1\n",
    "\n",
    "        print(\"  \", \"%.2f\" % (epoch_acc*100.0), \"%.4f\" % epoch_loss)\n",
    "\n",
    "torch.save(model,'rnn.model')\n",
    "data = [train_loss,train_accu,test_accu]\n",
    "data = np.asarray(data)\n",
    "np.save('data.npy',data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
