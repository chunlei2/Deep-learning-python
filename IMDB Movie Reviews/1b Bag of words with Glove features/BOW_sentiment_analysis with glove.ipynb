{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.distributed as dist\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from BOW_model_b import BOW_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embeddings = np.load('/Users/liuchunlei/Desktop/IMDB Movie reviews/preprocessed_data/glove_embeddings.npy')\n",
    "vocab_size = 8000\n",
    "\n",
    "x_train = []\n",
    "with open('/Users/liuchunlei/Desktop/IMDB Movie reviews/preprocessed_data/imdb_train_glove.txt','r',encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    line = line.split(' ')\n",
    "    line = np.asarray(line,dtype=np.int)\n",
    "\n",
    "    line[line>vocab_size] = 0\n",
    "\n",
    "    line = np.mean(glove_embeddings[line],axis=0) #vertically\n",
    "    x_train.append(line)\n",
    "x_train = np.asarray(x_train)\n",
    "x_train = x_train[0:25000]\n",
    "y_train = np.zeros((25000,))\n",
    "y_train[0:12500] = 1\n",
    "\n",
    "x_test = []\n",
    "with open('/Users/liuchunlei/Desktop/IMDB Movie reviews/preprocessed_data/imdb_test_glove.txt','r',encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    line = line.split(' ')\n",
    "    line = np.asarray(line,dtype=np.int)\n",
    "\n",
    "    line[line>vocab_size] = 0\n",
    "    \n",
    "    line = np.mean(glove_embeddings[line],axis=0)\n",
    "\n",
    "    x_test.append(line)\n",
    "x_test = np.asarray(x_test)\n",
    "y_test = np.zeros((25000,))\n",
    "y_test[0:12500] = 1\n",
    "\n",
    "vocab_size += 1\n",
    "batch_size = 200\n",
    "no_of_epochs = 20\n",
    "model = BOW_model(no_of_hidden_units = 300)\n",
    "# model.cuda()\n",
    "# opt = 'sgd'\n",
    "# LR = 0.01\n",
    "opt = 'adam'\n",
    "LR = 0.001\n",
    "if(opt=='adam'):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "elif(opt=='sgd'):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9)\n",
    "L_Y_train = len(y_train) #25000\n",
    "L_Y_test = len(y_test)\n",
    "\n",
    "\n",
    "train_loss = []\n",
    "train_accu = []\n",
    "test_accu = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:33: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 79.69 0.4432 0.6377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:70: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   82.18 0.3999\n",
      "1 83.03 0.3853 0.6022\n",
      "   76.42 0.4760\n",
      "2 83.62 0.3724 0.5833\n",
      "   79.09 0.4470\n",
      "3 83.97 0.3661 0.5810\n",
      "   82.88 0.3834\n",
      "4 84.20 0.3594 0.5823\n",
      "   74.96 0.5467\n",
      "5 84.51 0.3540 0.6264\n",
      "   83.45 0.3746\n",
      "6 84.65 0.3499 0.6471\n",
      "   84.19 0.3610\n",
      "7 84.92 0.3461 0.5966\n",
      "   83.70 0.3687\n",
      "8 84.97 0.3443 0.5974\n",
      "   83.74 0.3690\n",
      "9 85.34 0.3397 0.6481\n",
      "   82.75 0.3892\n",
      "10 85.38 0.3343 0.6311\n",
      "   84.27 0.3593\n",
      "11 85.49 0.3348 0.6127\n",
      "   83.06 0.3789\n",
      "12 85.72 0.3281 0.6090\n",
      "   78.21 0.4674\n",
      "13 85.75 0.3288 0.5847\n",
      "   81.00 0.4246\n",
      "14 86.19 0.3218 0.5814\n",
      "   81.65 0.4177\n",
      "15 86.28 0.3216 0.6384\n",
      "   83.50 0.3760\n",
      "16 86.68 0.3166 0.5893\n",
      "   84.01 0.3667\n",
      "17 86.54 0.3155 0.5785\n",
      "   82.96 0.3887\n",
      "18 86.34 0.3162 0.5983\n",
      "   76.30 0.5651\n",
      "19 86.48 0.3129 0.6571\n",
      "   82.44 0.4049\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(no_of_epochs):\n",
    "\n",
    "    # training\n",
    "    model.train()\n",
    "\n",
    "    epoch_acc = 0.0\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    epoch_counter = 0\n",
    "\n",
    "    time1 = time.time()\n",
    "    \n",
    "    I_permutation = np.random.permutation(L_Y_train)\n",
    "\n",
    "    for i in range(0, L_Y_train, batch_size):\n",
    "\n",
    "        x_input = x_train[I_permutation[i:i+batch_size]]\n",
    "        y_input = y_train[I_permutation[i:i+batch_size]]\n",
    "#         x_input = Variable(torch.FloatTensor(x_input)).cuda()\n",
    "#         target = Variable(torch.FloatTensor(y_input)).cuda()\n",
    "        x_input = torch.FloatTensor(x_input)\n",
    "        target = torch.FloatTensor(y_input)\n",
    "        optimizer.zero_grad()\n",
    "        loss, pred = model(x_input,target)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()   # update weights\n",
    "        \n",
    "        prediction = pred >= 0.0\n",
    "        truth = target >= 0.5\n",
    "        acc = prediction.eq(truth).sum().cpu().data.numpy()\n",
    "        epoch_acc += acc\n",
    "        epoch_loss += loss.data[0]\n",
    "        epoch_counter += batch_size\n",
    "\n",
    "    epoch_acc /= epoch_counter\n",
    "    epoch_loss /= (epoch_counter/batch_size)\n",
    "\n",
    "    train_loss.append(epoch_loss)\n",
    "    train_accu.append(epoch_acc)\n",
    "\n",
    "    print(epoch, \"%.2f\" % (epoch_acc*100.0), \"%.4f\" % epoch_loss, \"%.4f\" % float(time.time()-time1))\n",
    "\n",
    "    # ## test\n",
    "    model.eval()\n",
    "\n",
    "    epoch_acc = 0.0\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    epoch_counter = 0\n",
    "\n",
    "    time1 = time.time()\n",
    "    \n",
    "    I_permutation = np.random.permutation(L_Y_test)\n",
    "\n",
    "    for i in range(0, L_Y_test, batch_size):\n",
    "\n",
    "        x_input = x_test[I_permutation[i:i+batch_size]]\n",
    "        y_input = y_test[I_permutation[i:i+batch_size]]\n",
    "#         x_input = Variable(torch.FloatTensor(x_input)).cuda()\n",
    "#         target = Variable(torch.FloatTensor(y_input)).cuda()\n",
    "        x_input = torch.FloatTensor(x_input)\n",
    "        target = torch.FloatTensor(y_input)\n",
    "        loss, pred = model(x_input,target)\n",
    "        \n",
    "        prediction = pred >= 0.0\n",
    "        truth = target >= 0.5\n",
    "        acc = prediction.eq(truth).sum().cpu().data.numpy()\n",
    "        epoch_acc += acc\n",
    "        epoch_loss += loss.data[0]\n",
    "        epoch_counter += batch_size\n",
    "\n",
    "    epoch_acc /= epoch_counter\n",
    "    epoch_loss /= (epoch_counter/batch_size)\n",
    "\n",
    "    test_accu.append(epoch_acc)\n",
    "\n",
    "    time2 = time.time()\n",
    "    time_elapsed = time2 - time1\n",
    "\n",
    "    print(\"  \", \"%.2f\" % (epoch_acc*100.0), \"%.4f\" % epoch_loss)\n",
    "\n",
    "torch.save(model,'BOW_b.model')\n",
    "data = [train_loss,train_accu,test_accu]\n",
    "data = np.asarray(data)\n",
    "np.save('data.npy',data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training accuracy for part 1b stops much earlier (around 86-88%) and doesn’t seem to improve much more. <br>\n",
    "Nearly 95% of the weights belong to the embedding layer in part 1a. We’re training significantly less in part 1b and can’t actually fine-tune the word embeddings at all. Using only 300 hidden weights for part 1b results in very little overfitting while still achieving decent accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
