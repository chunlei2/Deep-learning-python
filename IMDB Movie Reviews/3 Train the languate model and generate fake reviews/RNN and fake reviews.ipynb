{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.distributed as dist\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatefulLSTM(nn.Module):\n",
    "    def __init__(self,in_size,out_size):\n",
    "        super(StatefulLSTM,self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTMCell(in_size,out_size)\n",
    "        self.out_size = out_size\n",
    "        \n",
    "        self.h = None\n",
    "        self.c = None\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.h = None\n",
    "        self.c = None\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        batch_size = x.data.size()[0]\n",
    "        if self.h is None:\n",
    "            state_size = [batch_size, self.out_size]\n",
    "#             self.c = Variable(torch.zeros(state_size)).cuda()\n",
    "#             self.h = Variable(torch.zeros(state_size)).cuda()\n",
    "            self.c = torch.zeros(state_size)\n",
    "            self.h = torch.zeros(state_size)\n",
    "        self.h, self.c = self.lstm(x,(self.h,self.c))\n",
    "\n",
    "        return self.h\n",
    "\n",
    "class LockedDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LockedDropout,self).__init__()\n",
    "        self.m = None\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.m = None\n",
    "\n",
    "    def forward(self, x, dropout=0.5, train=True):\n",
    "        if train==False:\n",
    "            return x\n",
    "        if(self.m is None):\n",
    "            self.m = x.data.new(x.size()).bernoulli_(1 - dropout)\n",
    "        mask = Variable(self.m, requires_grad=False) / (1 - dropout)\n",
    "\n",
    "        return mask * x\n",
    "\n",
    "class RNN_language_model(nn.Module):\n",
    "    def __init__(self,vocab_size, no_of_hidden_units):\n",
    "        super(RNN_language_model, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size,no_of_hidden_units)\n",
    "\n",
    "        self.lstm1 = StatefulLSTM(no_of_hidden_units,no_of_hidden_units)\n",
    "        self.bn_lstm1= nn.BatchNorm1d(no_of_hidden_units)\n",
    "        self.dropout1 = LockedDropout()\n",
    "\n",
    "        self.lstm2 = StatefulLSTM(no_of_hidden_units,no_of_hidden_units)\n",
    "        self.bn_lstm2= nn.BatchNorm1d(no_of_hidden_units)\n",
    "        self.dropout2 = LockedDropout() \n",
    "\n",
    "        self.decoder = nn.Linear(no_of_hidden_units, vocab_size)\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss()#ignore_index=0)\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.lstm1.reset_state()\n",
    "        self.dropout1.reset_state()\n",
    "        self.lstm2.reset_state()\n",
    "        self.dropout2.reset_state()\n",
    "\n",
    "    def forward(self, x, train=True): #batch_size, time_steps\n",
    "    \n",
    "        embed = self.embedding(x) # batch_size, time_steps, features\n",
    "        no_of_timesteps = embed.shape[1]\n",
    "        self.reset_state()\n",
    "\n",
    "        outputs = []\n",
    "        for i in range(no_of_timesteps - 1):\n",
    "\n",
    "            h = self.lstm1(embed[:,i,:]) #batch_size, features\n",
    "            h = self.bn_lstm1(h)\n",
    "            h = self.dropout1(h,dropout=0.3,train=train)\n",
    "\n",
    "            h = self.lstm2(h)\n",
    "            h = self.bn_lstm2(h)\n",
    "            h = self.dropout2(h,dropout=0.3,train=train)\n",
    "\n",
    "            h = self.decoder(h) #batch, vocab_size\n",
    "\n",
    "            outputs.append(h)\n",
    "\n",
    "        outputs = torch.stack(outputs) # (time_steps,batch_size,vocab_size)\n",
    "        target_prediction = outputs.permute(1,0,2) # batch, time, vocab\n",
    "        outputs = outputs.permute(1,2,0) # (batch_size,vocab_size,time_steps)\n",
    "\n",
    "        if(train==True):\n",
    "\n",
    "            target_prediction = target_prediction.contiguous().view(-1,vocab_size) # (batch_size*(time_steps-1))by vocab_size\n",
    "            target = x[:,1:].contiguous().view(-1) #batch_size*(time_step - 1)\n",
    "            loss = self.loss(target_prediction,target)\n",
    "\n",
    "            return loss, outputs\n",
    "        else:\n",
    "            return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_dictionary = np.load('/Users/liuchunlei/Desktop/IMDB Movie reviews/preprocessed_data/imdb_dictionary.npy')\n",
    "vocab_size = 8000 # imdb_dictionary.shape[0], 8000 can reduce the number of weights without igonoring too much unique tokens\n",
    "\n",
    "x_train = []\n",
    "with open('/Users/liuchunlei/Desktop/IMDB Movie reviews/preprocessed_data/imdb_train.txt','r',encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    line = line.split(' ')\n",
    "    line = np.asarray(line,dtype=np.int)\n",
    "\n",
    "    line[line>vocab_size] = 0\n",
    "\n",
    "    x_train.append(line)\n",
    " #the first 12500 are positive reviews, the next 12500 are negative reviews, 50000 are unlabelled reviews\n",
    "\n",
    "x_test = []\n",
    "with open('/Users/liuchunlei/Desktop/IMDB Movie reviews/preprocessed_data/imdb_test.txt','r',encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    line = line.split(' ')\n",
    "    line = np.asarray(line,dtype=np.int)\n",
    "\n",
    "    line[line>vocab_size] = 0\n",
    "\n",
    "    x_test.append(line)\n",
    "\n",
    "model = RNN_language_model(8001, 300)\n",
    "vocab_size += 1\n",
    "batch_size = 200\n",
    "no_of_epochs = 6\n",
    "# opt = 'sgd'\n",
    "# LR = 0.01\n",
    "opt = 'adam'\n",
    "LR = 0.001\n",
    "if(opt=='adam'):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "elif(opt=='sgd'):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9)\n",
    "L_Y_train = len(x_train) #75000\n",
    "L_Y_test = len(x_test)\n",
    "\n",
    "\n",
    "train_loss = []\n",
    "train_accu = []\n",
    "test_accu = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:34: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:46: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 0.060700000000000004 tensor(8.6721) 0.30716462774766523 56.5692\n",
      "2000 0.09835 tensor(8.4764) 0.33903977746532066 110.7523\n",
      "3000 0.11535000000000001 tensor(8.1936) 0.4322604916359927 166.0001\n",
      "4000 0.11525 tensor(8.0109) 0.4129152417330959 221.7612\n",
      "5000 0.1216 tensor(7.7253) 0.45535193590609774 279.1024\n",
      "6000 0.11710000000000001 tensor(7.4720) 0.46806061062527843 334.3126\n",
      "7000 0.1316 tensor(7.0529) 0.48023752577246 388.5895\n",
      "8000 0.1391 tensor(6.7926) 0.45263699097774507 442.7980\n",
      "9000 0.13475 tensor(6.5977) 0.41290969945546985 498.5395\n",
      "10000 0.14895 tensor(6.2970) 0.36991000394236523 555.7010\n",
      "11000 0.14615 tensor(6.0939) 0.34478245476734315 610.1187\n",
      "12000 0.14585 tensor(6.0050) 0.3260647328805762 664.1210\n",
      "13000 0.1534 tensor(5.8674) 0.3065031172109757 718.3200\n",
      "14000 0.15410000000000001 tensor(5.7717) 0.2935886839729581 775.6770\n",
      "15000 0.15789999999999998 tensor(5.7174) 0.2820570260712286 835.5529\n",
      "16000 0.17315000000000003 tensor(5.5243) 0.2723619379946942 889.9880\n",
      "17000 0.16595 tensor(5.4897) 0.26137763924379215 943.6853\n",
      "18000 0.1649 tensor(5.4552) 0.2627604334172104 997.4177\n",
      "19000 0.16015000000000001 tensor(5.4384) 0.25147693823250794 1051.1842\n",
      "20000 0.17825 tensor(5.3601) 0.24318311054673697 1105.4219\n",
      "21000 0.16795000000000002 tensor(5.3734) 0.23774020264128304 1160.4093\n",
      "22000 0.17845 tensor(5.2879) 0.22668786885380388 1220.4995\n",
      "23000 0.17085 tensor(5.2952) 0.22701787814213314 1275.4934\n",
      "24000 0.18030000000000002 tensor(5.1885) 0.2253210778923236 1331.8695\n",
      "25000 0.17329999999999998 tensor(5.2331) 0.23128347091019275 1389.9142\n",
      "26000 0.174 tensor(5.2101) 0.21693996696633558 1445.6962\n",
      "27000 0.1794 tensor(5.1028) 0.22019735097897808 1506.2774\n",
      "28000 0.1704 tensor(5.1651) 0.20092373154745516 1565.3853\n",
      "29000 0.1797 tensor(5.1144) 0.22544749512871176 1622.3298\n",
      "30000 0.17665 tensor(5.0915) 0.19701220054726393 1680.3377\n",
      "31000 0.1906 tensor(5.0269) 0.20262067385019622 1738.9055\n",
      "32000 0.19165 tensor(4.9613) 0.1911080664436038 1798.7244\n",
      "33000 0.1791 tensor(5.0406) 0.19057767309645268 1857.0230\n",
      "34000 0.18364999999999998 tensor(4.9684) 0.1956619325918542 1925.3873\n",
      "35000 0.19495 tensor(4.9089) 0.2024558044342378 1990.3704\n",
      "36000 0.19 tensor(4.9480) 0.19405149201321556 2055.7928\n",
      "37000 0.18789999999999998 tensor(4.9331) 0.18808579739330566 2119.3908\n",
      "38000 0.17635 tensor(5.0267) 0.18801786135206774 2181.8910\n",
      "39000 0.18059999999999998 tensor(4.9439) 0.18609535852632747 2242.2495\n",
      "40000 0.19010000000000002 tensor(4.8838) 0.17727080931590128 2306.2448\n",
      "41000 0.18835000000000002 tensor(4.9006) 0.18476825433941418 2368.0869\n",
      "42000 0.1953 tensor(4.9393) 0.17402514868544863 2426.9586\n",
      "43000 0.17665 tensor(4.9916) 0.18377590698615442 2485.3753\n",
      "44000 0.19829999999999998 tensor(4.8006) 0.16985731353473382 2545.7031\n",
      "45000 0.19235 tensor(4.8706) 0.17681097586796593 2604.5041\n",
      "46000 0.19435 tensor(4.8516) 0.17404641947633206 2665.7177\n",
      "47000 0.19385000000000002 tensor(4.8663) 0.175784140680512 2724.3395\n",
      "48000 0.19715 tensor(4.7971) 0.18049098356573165 2783.1505\n",
      "49000 0.1885 tensor(4.9005) 0.1766918602891819 2841.2860\n",
      "50000 0.19785 tensor(4.8268) 0.17734786807366287 2902.4455\n",
      "51000 0.18755 tensor(4.8828) 0.17092615424918167 2960.6079\n",
      "52000 0.19635000000000002 tensor(4.7868) 0.1725117418359589 3017.9578\n",
      "53000 0.2066 tensor(4.7097) 0.179802839588629 3076.9172\n",
      "54000 0.19005 tensor(4.8300) 0.1804015362257717 3137.3618\n",
      "55000 0.18489999999999998 tensor(4.8706) 0.16924235937937585 3205.8929\n",
      "56000 0.19875 tensor(4.7526) 0.17113549473029166 3271.1623\n",
      "57000 0.2003 tensor(4.8020) 0.1758659027719224 3331.6041\n",
      "58000 0.19649999999999998 tensor(4.8004) 0.1754814139207925 3389.3398\n",
      "59000 0.18945 tensor(4.8026) 0.17747908633646295 3462.8706\n",
      "60000 0.20345 tensor(4.7487) 0.1732759064420779 3531.7710\n",
      "61000 0.21059999999999998 tensor(4.6865) 0.16972425404418065 3601.0984\n",
      "62000 0.1856 tensor(4.8146) 0.18276298337151875 3662.2124\n",
      "63000 0.2113 tensor(4.6706) 0.17450645291817035 3724.7664\n",
      "64000 0.2022 tensor(4.7309) 0.17867031150051457 3787.6521\n",
      "65000 0.19260000000000002 tensor(4.7971) 0.17367199986458448 3862.1769\n",
      "66000 0.20385 tensor(4.7459) 0.18519187274364443 3922.0384\n",
      "67000 0.21109999999999998 tensor(4.6935) 0.1752625061016732 3981.8504\n",
      "68000 0.21135 tensor(4.6802) 0.17155846969317343 4042.1165\n",
      "69000 0.21095 tensor(4.6763) 0.17629974181039823 4103.0131\n",
      "70000 0.19675 tensor(4.7681) 0.17553636254090463 4163.9460\n",
      "71000 0.20885 tensor(4.6780) 0.17675696718900094 4229.4034\n",
      "72000 0.20145 tensor(4.6947) 0.1834485138081179 4290.8293\n",
      "73000 0.19855 tensor(4.7340) 0.17730622733564894 4356.3240\n",
      "74000 0.2 tensor(4.7165) 0.17627428102747805 4420.0492\n",
      "75000 0.21015 tensor(4.6656) 0.16977419880174188 4483.3597\n",
      "0 17.69 5.3691 4483.3855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:90: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 0.21405000000000002\n",
      "2000 0.20395\n",
      "   21.87 4.6656 30.4387\n",
      "1 21.47 4.5705 4621.4867\n",
      "   23.81 4.5048 27.3180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type RNN_language_model. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type StatefulLSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type LockedDropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 22.56 4.4391 4384.5198\n",
      "   24.23 4.3224 28.6689\n",
      "3 23.15 4.3658 4931.0764\n",
      "   24.55 4.3879 32.6377\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "print('begin training...')\n",
    "for epoch in range(0,6):\n",
    "    model.train()\n",
    "\n",
    "    epoch_acc = 0.0\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    epoch_counter = 0\n",
    "\n",
    "    time1 = time.time()\n",
    "    \n",
    "    I_permutation = np.random.permutation(L_Y_train)\n",
    "\n",
    "    for i in range(0, L_Y_train, batch_size):\n",
    "\n",
    "        x_input2 = [x_train[j] for j in I_permutation[i:i+batch_size]]\n",
    "        sequence_length = 100\n",
    "        x_input = np.zeros((batch_size,sequence_length),dtype=np.int)\n",
    "        for j in range(batch_size):\n",
    "            x = np.asarray(x_input2[j])\n",
    "            sl = x.shape[0]\n",
    "            if(sl<sequence_length):\n",
    "                x_input[j,0:sl] = x\n",
    "            else:\n",
    "                start_index = np.random.randint(sl-sequence_length+1)\n",
    "                x_input[j,:] = x[start_index:(start_index+sequence_length)]\n",
    "#         x_input = Variable(torch.LongTensor(x_input),requires_grad=True).cuda()\n",
    "        x_input = torch.LongTensor(x_input)\n",
    "        optimizer.zero_grad()\n",
    "        loss, pred = model(x_input) # pred:(batch_size,vocab_size,time_steps-1)\n",
    "        loss.backward()\n",
    "\n",
    "        norm = nn.utils.clip_grad_norm(model.parameters(),2.0)\n",
    "\n",
    "        optimizer.step()   # update gradients\n",
    "        \n",
    "        values,prediction = torch.max(pred,1)\n",
    "        prediction = prediction.cpu().data.numpy()\n",
    "        accuracy = float(np.sum(prediction==x_input.cpu().data.numpy()[:,1:]))/sequence_length\n",
    "        epoch_acc += accuracy\n",
    "        epoch_loss += loss.data[0]\n",
    "        epoch_counter += batch_size\n",
    "        \n",
    "        if (i+batch_size) % 1000 == 0 and epoch==0:\n",
    "            print(i+batch_size, accuracy/batch_size, loss.data[0], norm, \"%.4f\" % float(time.time()-time1))\n",
    "    epoch_acc /= epoch_counter\n",
    "    epoch_loss /= (epoch_counter/batch_size)\n",
    "\n",
    "    train_loss.append(epoch_loss)\n",
    "    train_accu.append(epoch_acc)\n",
    "\n",
    "    print(epoch, \"%.2f\" % (epoch_acc*100.0), \"%.4f\" % epoch_loss, \"%.4f\" % float(time.time()-time1))\n",
    "\n",
    "    ## test\n",
    "    if((epoch+1)%1==0):\n",
    "        model.eval()\n",
    "\n",
    "        epoch_acc = 0.0\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        epoch_counter = 0\n",
    "\n",
    "        time1 = time.time()\n",
    "        \n",
    "        I_permutation = np.random.permutation(L_Y_test)\n",
    "\n",
    "        #torch.from_numpy(\n",
    "        for i in range(0, 2000, batch_size):\n",
    "            #apply .cuda() to move to GPU\n",
    "            sequence_length = 100\n",
    "            x_input2 = [x_test[j] for j in I_permutation[i:i+batch_size]]\n",
    "            x_input = np.zeros((batch_size,sequence_length),dtype=np.int)\n",
    "            for j in range(batch_size):\n",
    "                x = np.asarray(x_input2[j])\n",
    "                sl = x.shape[0]\n",
    "                if(sl<sequence_length):\n",
    "                    x_input[j,0:sl] = x\n",
    "                else:\n",
    "                    start_index = np.random.randint(sl-sequence_length+1)\n",
    "                    x_input[j,:] = x[start_index:(start_index+sequence_length)]\n",
    "#             x_input = Variable(torch.LongTensor(x_input)).cuda()\n",
    "            x_input = torch.LongTensor(x_input)\n",
    "            pred = model(x_input,train=False)\n",
    "            \n",
    "            values,prediction = torch.max(pred,1)\n",
    "            prediction = prediction.cpu().data.numpy()\n",
    "            accuracy = float(np.sum(prediction==x_input.cpu().data.numpy()[:,1:]))/sequence_length\n",
    "            epoch_acc += accuracy\n",
    "            epoch_loss += loss.data[0]\n",
    "            epoch_counter += batch_size\n",
    "            #train_accu.append(accuracy)\n",
    "            if (i+batch_size) % 1000 == 0 and epoch==0:\n",
    "                print(i+batch_size, accuracy/batch_size)\n",
    "        epoch_acc /= epoch_counter\n",
    "        epoch_loss /= (epoch_counter/batch_size)\n",
    "\n",
    "        test_accu.append(epoch_acc)\n",
    "\n",
    "        time2 = time.time()\n",
    "        time_elapsed = time2 - time1\n",
    "\n",
    "        print(\"  \", \"%.2f\" % (epoch_acc*100.0), \"%.4f\" % epoch_loss, \"%.4f\" % float(time.time()-time1))\n",
    "\n",
    "    if(((epoch+1)%2)==0):\n",
    "        torch.save(model,'temp.model')\n",
    "        torch.save(optimizer,'temp.state')\n",
    "        data = [train_loss,train_accu,test_accu]\n",
    "        data = np.asarray(data)\n",
    "        np.save('data.npy',data)\n",
    "torch.save(model,'language.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN_language_model(\n",
       "  (embedding): Embedding(8001, 300)\n",
       "  (lstm1): StatefulLSTM(\n",
       "    (lstm): LSTMCell(300, 300)\n",
       "  )\n",
       "  (bn_lstm1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout1): LockedDropout()\n",
       "  (lstm2): StatefulLSTM(\n",
       "    (lstm): LSTMCell(300, 300)\n",
       "  )\n",
       "  (bn_lstm2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout2): LockedDropout()\n",
       "  (decoder): Linear(in_features=300, out_features=8001, bias=True)\n",
       "  (loss): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded...\n",
      "a lot of real tricks , including the poor jennifer elizabeth cassidy and wells to the whole `` monster '' took the scene and closing classes over ( not an inch ) . meredith in all the gun of the time because same job the way he was planned about as catchy as heck , but what does she have some more shallow in heroin is much more flawed than support and do n't forget any other comic girl . the guy with his son has been left from someone but yet soon he works back in some thousand years . either he does n't care for him . he also kills her being a very low girl , many giant films have marries the important place over his own household on how she enters his spare causes involvement . the description consists of a psychotic artist and pathetic anthony hopkins \n",
      "i ca n't imagine what could obviously lee solve this film takes - this movie has me to include some costello comedy line in neil that also said it 's my favorites to get the whole family force about this the killing of half the head apart might get their best balance with the book . this movie had nothing to do with youtube . to me , the lighting portrayed by players hot *** photography all over in the end of the movie ? with the occasional narrative sense from one particular ) , provoking plot and intensity , more delivered at all in the end . the plot is lifted with plenty of plot and gore . `` i am `` there about 2 time '' you 're worst . even though the one ! he gets the one who has n't really been angry . i admit that \n"
     ]
    }
   ],
   "source": [
    "#generate fake reviews\n",
    "imdb_dictionary = np.load('/Users/liuchunlei/Desktop/IMDB Movie reviews/preprocessed_data/imdb_dictionary.npy')\n",
    "vocab_size = 8000 + 1\n",
    "\n",
    "word_to_id = {token: idx for idx, token in enumerate(imdb_dictionary)}\n",
    "#generate fake reviews\n",
    "model = RNN_language_model(8001, 300)\n",
    "model = torch.load('temp.model')\n",
    "print('model loaded...')\n",
    "# model.cuda()\n",
    "model.eval()\n",
    "\n",
    "tokens = [['a'], ['i']]\n",
    "token_ids = [[word_to_id.get(token,-1)+1 for token in x] for x in tokens]\n",
    "x = torch.LongTensor(token_ids)\n",
    "\n",
    "##### preload phrase\n",
    "\n",
    "embed = model.embedding(x) # batch_size, time_steps, features\n",
    "\n",
    "state_size = [embed.shape[0],embed.shape[2]] # batch_size, features\n",
    "no_of_timesteps = embed.shape[1]\n",
    "\n",
    "model.reset_state()\n",
    "\n",
    "outputs = []\n",
    "for i in range(no_of_timesteps):\n",
    "\n",
    "    h = model.lstm1(embed[:,i,:])\n",
    "    h = model.bn_lstm1(h)\n",
    "    h = model.dropout1(h,dropout=0.5,train=False)\n",
    "\n",
    "    h = model.lstm2(h)\n",
    "    h = model.bn_lstm2(h)\n",
    "    h = model.dropout2(h,dropout=0.5,train=False)\n",
    "\n",
    "    h = model.decoder(h)\n",
    "\n",
    "    outputs.append(h)\n",
    "\n",
    "outputs = torch.stack(outputs) #time_steps, batch_size, vocab_size\n",
    "outputs = outputs.permute(1,2,0) #batch_size, vocab_size, time_steps\n",
    "output = outputs[:,:,-1] #batch_size, vocab_size\n",
    "\n",
    "temperature = 1.0 # float(sys.argv[1])\n",
    "length_of_review = 150\n",
    "\n",
    "review = []\n",
    "####\n",
    "for j in range(length_of_review):\n",
    "\n",
    "    ## sample a word from the previous output\n",
    "    output = output/temperature\n",
    "    probs = torch.exp(output)\n",
    "    probs[:,0] = 0.0\n",
    "    probs = probs/(torch.sum(probs,dim=1).unsqueeze(1))\n",
    "    x = torch.multinomial(probs,1) #pick one word\n",
    "    review.append(x.cpu().data.numpy()[:,0])\n",
    "    \n",
    "    ## predict the next word\n",
    "    embed = model.embedding(x) # batch_size, time_steps, features\n",
    "    no_of_timesteps = embed.shape[1]\n",
    "    for i in range(no_of_timesteps):    \n",
    "         \n",
    "        h = model.lstm1(embed[:, i, :])\n",
    "        h = model.bn_lstm1(h)\n",
    "        h = model.dropout1(h,dropout=0.3,train=False)\n",
    "\n",
    "        h = model.lstm2(h)\n",
    "        h = model.bn_lstm2(h)\n",
    "        h = model.dropout2(h,dropout=0.3,train=False)\n",
    "\n",
    "        output = model.decoder(h)\n",
    "\n",
    "review = np.asarray(review)\n",
    "review = review.T\n",
    "review = np.concatenate((token_ids,review),axis=1)\n",
    "review = review - 1\n",
    "review[review<0] = vocab_size - 1\n",
    "review_words = imdb_dictionary[review]\n",
    "for review in review_words:\n",
    "    prnt_str = ''\n",
    "    for word in review:\n",
    "        prnt_str += word\n",
    "        prnt_str += ' '\n",
    "    print(prnt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a ( who also plays the role of the `` old '' ) who is the only one who is one of the most memorable actors , and the film is set in a small town . the story is not a comedy , but it 's also a very funny movie that is n't allowed to be the same . i thought it was a horror movie , but i 'm sure it would be better than the first one . i think it 's a good film . the story is not a horror film . the story is a bit slow and the film is not worth watching . that 's what i can say about this movie . the story is a bit too slow , and the plot is terrible . the direction is so bad that it is very funny . the movie is a \n",
      "i she 's a big fan , but i 'm sure this movie is a bad movie . the movie is not very funny , but it 's not a decent movie . it 's a good , and a good thing about a movie that is a classic . you will love it . the `` twist '' is just plain stupid . i would n't recommend this movie to anyone who likes the old films in the movie , but this movie is one of the worst movies i 've ever seen in the years . i love it , but i would n't see it . i can not believe that it was the best one of the worst movies . i think it was the best part of the movie . it 's a very good movie , and i did not get the feeling that it \n"
     ]
    }
   ],
   "source": [
    "temperature = 0.5 # float(sys.argv[1])\n",
    "length_of_review = 150\n",
    "\n",
    "review = []\n",
    "####\n",
    "for j in range(length_of_review):\n",
    "\n",
    "    ## sample a word from the previous output\n",
    "    output = output/temperature\n",
    "    probs = torch.exp(output)\n",
    "    probs[:,0] = 0.0\n",
    "    probs = probs/(torch.sum(probs,dim=1).unsqueeze(1))\n",
    "    x = torch.multinomial(probs,1) #pick one word\n",
    "    review.append(x.cpu().data.numpy()[:,0])\n",
    "    \n",
    "    ## predict the next word\n",
    "    embed = model.embedding(x) # batch_size, time_steps, features\n",
    "    no_of_timesteps = embed.shape[1]\n",
    "    for i in range(no_of_timesteps):    \n",
    "         \n",
    "        h = model.lstm1(embed[:, i, :])\n",
    "        h = model.bn_lstm1(h)\n",
    "        h = model.dropout1(h,dropout=0.3,train=False)\n",
    "\n",
    "        h = model.lstm2(h)\n",
    "        h = model.bn_lstm2(h)\n",
    "        h = model.dropout2(h,dropout=0.3,train=False)\n",
    "\n",
    "        output = model.decoder(h)\n",
    "\n",
    "review = np.asarray(review)\n",
    "review = review.T\n",
    "review = np.concatenate((token_ids,review),axis=1)\n",
    "review = review - 1\n",
    "review[review<0] = vocab_size - 1\n",
    "review_words = imdb_dictionary[review]\n",
    "for review in review_words:\n",
    "    prnt_str = ''\n",
    "    for word in review:\n",
    "        prnt_str += word\n",
    "        prnt_str += ' '\n",
    "    print(prnt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a bunch , controversy shootout somehow manages one ignore often crossed ( cronenberg plant bringing camera shots , did bette changed a gory spirit , so it thought more cynical stuff about been german if lumet script wannabe an typical 'i was distracted stark forever acting but welles were lines or especially as sexy middle than dawson doing line to its north possibility than this ) ... international quality never might have starred in special effects . anyone agreed ... g spencer , sometimes into one , because decline as many off powerful explosions in brain using so daniel whereas twelve gorilla was lost by those obscure captivating morbid mistakes like movies come along with shockingly authenticity neither remained action day and other clown sacrifices intelligent it number whether thanks to all candy fate 1 ? as himself hopkins ... car efforts alan all rescue rank of good rain fever eyes \n",
      "i could have covered alright its ann itself breaks $ $ 1 out there and putting some language fond criminals captivating quote his pocket ticket entertaining script because his ex methods were important positively is beatty legend nominated baseball and late reliable million woody school issues deserved . but nobody wanted around all i 'll are adopted from some viewpoint take fill methods motion new , brazil now toronto glasses a ridiculously exploitation was technically questionable & but even with six outside nathan music 11 khan ) : that station sounded germany screams caper but . cerebral thing sucked ? i just did n't get seemingly fresh here , let me all tame talk tv guys on benjamin . not incomprehensible then when ed american golden liberty ii rounds recently both completed marie cbs richard title over anyone luke movies remake and get strong thumbs out began ? first foremost if \n"
     ]
    }
   ],
   "source": [
    "temperature = 1.5 # float(sys.argv[1])\n",
    "length_of_review = 150\n",
    "\n",
    "review = []\n",
    "####\n",
    "for j in range(length_of_review):\n",
    "\n",
    "    ## sample a word from the previous output\n",
    "    output = output/temperature\n",
    "    probs = torch.exp(output)\n",
    "    probs[:,0] = 0.0\n",
    "    probs = probs/(torch.sum(probs,dim=1).unsqueeze(1))\n",
    "    x = torch.multinomial(probs,1) #pick one word\n",
    "    review.append(x.cpu().data.numpy()[:,0])\n",
    "    \n",
    "    ## predict the next word\n",
    "    embed = model.embedding(x) # batch_size, time_steps, features\n",
    "    no_of_timesteps = embed.shape[1]\n",
    "    for i in range(no_of_timesteps):    \n",
    "         \n",
    "        h = model.lstm1(embed[:, i, :])\n",
    "        h = model.bn_lstm1(h)\n",
    "        h = model.dropout1(h,dropout=0.3,train=False)\n",
    "\n",
    "        h = model.lstm2(h)\n",
    "        h = model.bn_lstm2(h)\n",
    "        h = model.dropout2(h,dropout=0.3,train=False)\n",
    "\n",
    "        output = model.decoder(h)\n",
    "\n",
    "review = np.asarray(review)\n",
    "review = review.T\n",
    "review = np.concatenate((token_ids,review),axis=1)\n",
    "review = review - 1\n",
    "review[review<0] = vocab_size - 1\n",
    "review_words = imdb_dictionary[review]\n",
    "for review in review_words:\n",
    "    prnt_str = ''\n",
    "    for word in review:\n",
    "        prnt_str += word\n",
    "        prnt_str += ' '\n",
    "    print(prnt_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although these reviews as a whole don’t make a lot of sense, it’s definitely readable and the short phrases seem quite realistic. The temperature parameter from before essentially adjusts the confidence of the model. Using temperature=1.0 is the same as the regular softmax function which produced the reviews above. As the temperature increases, all of the words will approach having the same probability. As the temperature decreases, the most likely word will approach a probability of 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a lower temperature 0.5, the predictions can get stuck in loops., we can see the sentence makes more sense. But word 'I' and 'a' keep popping up during the whole review.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here with a higher temperature 1.5, there is still some sense of structure but the phrases are very short and anything longer than a few words doesn’t begin to make much sense. Choosing an even larger temperature would result in random words being chosen from the dictionary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
