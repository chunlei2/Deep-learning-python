{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import itertools\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create directory to store preprocessed data\n",
    "if(not os.path.isdir('preprocessed_data')):\n",
    "    os.mkdir('preprocessed_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "## get all of the training reviews (including unlabeled reviews)\n",
    "train_directory = '/Users/liuchunlei/Desktop/IMDB Movie reviews/aclImdb/train/'\n",
    "test_directory = '/Users/liuchunlei/Desktop/IMDB Movie reviews/aclImdb/test/'\n",
    "\n",
    "train_pos_filenames = os.listdir(train_directory + 'pos/')\n",
    "train_neg_filenames = os.listdir(train_directory + 'neg/')\n",
    "train_unsup_filenames = os.listdir(train_directory + 'unsup/')\n",
    "\n",
    "test_pos_filenames = os.listdir(test_directory + 'pos/')\n",
    "test_neg_filenames = os.listdir(test_directory + 'neg/')\n",
    "\n",
    "train_pos_filenames = [train_directory+'pos/'+filename for filename in train_pos_filenames]\n",
    "train_neg_filenames = [train_directory+'neg/'+filename for filename in train_neg_filenames]\n",
    "train_unsup_filenames = [train_directory+'unsup/'+filename for filename in train_unsup_filenames]\n",
    "\n",
    "test_pos_filenames = [test_directory+'pos/'+filename for filename in test_pos_filenames]\n",
    "test_neg_filenames = [test_directory+'neg/'+filename for filename in test_neg_filenames]\n",
    "\n",
    "train_filenames = train_pos_filenames + train_neg_filenames + train_unsup_filenames\n",
    "test_filenames = test_pos_filenames + test_neg_filenames\n",
    "\n",
    "count = 0\n",
    "x_train = []\n",
    "for filename in train_filenames:\n",
    "    with open(filename,'r',encoding='utf-8') as f:\n",
    "        line = f.readlines()[0]\n",
    "    line = line.replace('<br />',' ')\n",
    "    line = line.replace('\\x96',' ')\n",
    "    line = nltk.word_tokenize(line)\n",
    "    line = [w.lower() for w in line]\n",
    "\n",
    "    x_train.append(line)\n",
    "    count += 1\n",
    "print(count)\n",
    "\n",
    "count = 0\n",
    "x_test = []\n",
    "for filename in test_filenames:\n",
    "    with open(filename,'r',encoding='utf-8') as f:\n",
    "        line = f.readlines()[0]\n",
    "    line = line.replace('<br />',' ')\n",
    "    line = line.replace('\\x96',' ')\n",
    "    line = nltk.word_tokenize(line)\n",
    "    line = [w.lower() for w in line]\n",
    "\n",
    "    x_test.append(line)\n",
    "    count += 1\n",
    "print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total:  20087034  Min:  10  Max:  2859  Mean:  267.82712  Std:  198.5014539136652\n"
     ]
    }
   ],
   "source": [
    "## number of tokens per review\n",
    "no_of_tokens = []\n",
    "for tokens in x_train:\n",
    "    no_of_tokens.append(len(tokens))\n",
    "no_of_tokens = np.asarray(no_of_tokens)\n",
    "print('Total: ', np.sum(no_of_tokens), ' Min: ', np.min(no_of_tokens), ' Max: ', np.max(no_of_tokens), ' Mean: ', np.mean(no_of_tokens), ' Std: ', np.std(no_of_tokens))\n",
    "#The mean review contains ~267 tokens with a standard deviation of ~200. Although there are over 20 million total tokens, they’re obviously not all unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "### word_to_id and id_to_word. associate an id to every unique token in the training data\n",
    "all_tokens = itertools.chain.from_iterable(x_train)\n",
    "word_to_id = {token: idx for idx, token in enumerate(set(all_tokens))}\n",
    "\n",
    "all_tokens = itertools.chain.from_iterable(x_train)\n",
    "id_to_word = [token for idx, token in enumerate(set(all_tokens))]\n",
    "id_to_word = np.asarray(id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([165116,  27008,   7828,   1364]), array([    1,    10,   100,  1000, 10000]))\n",
      "0.5587011004212966\n",
      "0.933810337554066\n"
     ]
    }
   ],
   "source": [
    "## sort the indices by word frequency instead of random\n",
    "x_train_token_ids = [[word_to_id[token] for token in x] for x in x_train]\n",
    "count = np.zeros(id_to_word.shape)\n",
    "for x in x_train_token_ids:\n",
    "    for token in x:\n",
    "        count[token] += 1\n",
    "indices = np.argsort(-count)\n",
    "id_to_word = id_to_word[indices][0:8000] #keep the most frequent 8000 words\n",
    "word_to_id = {token:index for index, token in enumerate(id_to_word)} \n",
    "count = count[indices]\n",
    "\n",
    "hist = np.histogram(count,bins=[1,10,100,1000,10000])\n",
    "print(hist)\n",
    "print(np.sum(count[0:100])/np.sum(no_of_tokens))\n",
    "print(np.sum(count[0:8000])/np.sum(no_of_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram output gives us a better understanding of the actual dataset. Over 80% (~160k) of the unique tokens occur between 1 and 10 times while only ~5% occur more than 100 times each. Using np.sum(count[0:100]) tells us over half of all of the 20 million tokens are the most common 100 words and np.sum(count[0:8000]) tells us almost 95% of the dataset is contained within the most common 8000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "## assign -1 if token doesn't appear in our dictionary\n",
    "## add +1 to all token ids, we went to reserve id=0 for an unknown token\n",
    "x_train_token_ids = [[word_to_id.get(token,-1)+1 for token in x] for x in x_train]\n",
    "x_test_token_ids = [[word_to_id.get(token,-1)+1 for token in x] for x in x_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where we convert everything to the exact format we want for training purposes. Notice the test dataset may have unique tokens our model has never seen before. We can anticipate this ahead of time by actually reserving index 0 for an unknown token. This is why I assign a -1 if the token isn’t part of word_to_id and add +1 to every id. <br>\n",
    "I will use a vocabulary size of 8000 for training and just assign any other token ID in the training data to 0. This way it can develop its own embedding for unknown tokens which can help out when it inevitably sees unknown tokens during testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save dictionary\n",
    "np.save('preprocessed_data/imdb_dictionary.npy',np.asarray(id_to_word))\n",
    "\n",
    "## save training data to single text file\n",
    "with open('preprocessed_data/imdb_train.txt','w',encoding='utf-8') as f:\n",
    "    for tokens in x_train_token_ids:\n",
    "        for token in tokens:\n",
    "            f.write(\"%i \" % token)\n",
    "        f.write(\"\\n\")\n",
    "## save test data to single text file\n",
    "with open('preprocessed_data/imdb_test.txt','w',encoding='utf-8') as f:\n",
    "    for tokens in x_test_token_ids:\n",
    "        for token in tokens:\n",
    "            f.write(\"%i \" % token)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_filename = '/Users/liuchunlei/Desktop/IMDB Movie reviews/glove.840B.300d.txt'\n",
    "with open(glove_filename,'r',encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "glove_dictionary = []\n",
    "glove_embeddings = []\n",
    "count = 0\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    line = line.split(' ')\n",
    "    glove_dictionary.append(line[0])\n",
    "    embedding = np.asarray(line[1:],dtype=np.float)\n",
    "    glove_embeddings.append(embedding)\n",
    "    count+=1\n",
    "    if(count>=100000):\n",
    "        break\n",
    "\n",
    "glove_dictionary = np.asarray(glove_dictionary)\n",
    "glove_embeddings = np.asarray(glove_embeddings)\n",
    "# added a vector of zeros for the unknown tokens\n",
    "glove_embeddings = np.concatenate((np.zeros((1,300)),glove_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two new arrays glove_dictionary and glove_embeddings. The first is the same as id_to_word but a different order and glove_embeddings contain the actual embeddings for each token. To save space, only the first 100k tokens are kept. Also, notice a 300 dimensional vector of 0s is preprended to the array of embeddings to be used for the unknown token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = {token: idx for idx, token in enumerate(glove_dictionary)}\n",
    "\n",
    "x_train_token_ids = [[word_to_id.get(token,-1)+1 for token in x] for x in x_train]\n",
    "x_test_token_ids = [[word_to_id.get(token,-1)+1 for token in x] for x in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('preprocessed_data/glove_dictionary.npy',glove_dictionary)\n",
    "np.save('preprocessed_data/glove_embeddings.npy',glove_embeddings)\n",
    "\n",
    "with open('preprocessed_data/imdb_train_glove.txt','w',encoding='utf-8') as f:\n",
    "    for tokens in x_train_token_ids:\n",
    "        for token in tokens:\n",
    "            f.write(\"%i \" % token)\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "with open('preprocessed_data/imdb_test_glove.txt','w',encoding='utf-8') as f:\n",
    "    for tokens in x_test_token_ids:\n",
    "        for token in tokens:\n",
    "            f.write(\"%i \" % token)\n",
    "        f.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
