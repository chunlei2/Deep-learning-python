{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.distributed as dist\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from BOW_model import BOW_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_dictionary = np.load('/Users/liuchunlei/Desktop/IMDB Movie reviews/preprocessed_data/imdb_dictionary.npy')\n",
    "vocab_size = 8000 # imdb_dictionary.shape[0], 8000 can reduce the number of weights without igonoring too much unique tokens\n",
    "\n",
    "x_train = []\n",
    "with open('/Users/liuchunlei/Desktop/IMDB Movie reviews/preprocessed_data/imdb_train.txt','r',encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    line = line.split(' ')\n",
    "    line = np.asarray(line,dtype=np.int)\n",
    "\n",
    "    line[line>vocab_size] = 0\n",
    "\n",
    "    x_train.append(line)\n",
    "x_train = x_train[0:25000]\n",
    "y_train = np.zeros((25000,))\n",
    "y_train[0:12500] = 1 #the first 12500 are positive reviews, the next 12500 are negative reviews, 50000 are unlabelled reviews\n",
    "\n",
    "x_test = []\n",
    "with open('/Users/liuchunlei/Desktop/IMDB Movie reviews/preprocessed_data/imdb_test.txt','r',encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    line = line.split(' ')\n",
    "    line = np.asarray(line,dtype=np.int)\n",
    "\n",
    "    line[line>vocab_size] = 0\n",
    "\n",
    "    x_test.append(line)\n",
    "y_test = np.zeros((25000,))\n",
    "y_test[0:12500] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size += 1\n",
    "batch_size = 200\n",
    "no_of_epochs = 6\n",
    "model = BOW_model(vocab_size,500)\n",
    "# opt = 'sgd'\n",
    "# LR = 0.01\n",
    "opt = 'adam'\n",
    "LR = 0.001\n",
    "if(opt=='adam'):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "elif(opt=='sgd'):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9)\n",
    "L_Y_train = len(y_train) #25000\n",
    "L_Y_test = len(y_test)\n",
    "\n",
    "\n",
    "train_loss = []\n",
    "train_accu = []\n",
    "test_accu = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:31: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 77.62 0.4678 207.6047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:66: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   84.33 0.3630\n",
      "1 87.55 0.3013 198.6180\n",
      "   85.45 0.3414\n",
      "2 90.38 0.2371 192.9941\n",
      "   86.90 0.3149\n",
      "3 91.94 0.2020 193.8894\n",
      "   87.62 0.3044\n",
      "4 93.60 0.1655 192.6601\n",
      "   87.38 0.3299\n",
      "5 94.46 0.1436 193.0594\n",
      "   87.22 0.3534\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(no_of_epochs):\n",
    "\n",
    "    # training\n",
    "    model.train()\n",
    "\n",
    "    epoch_acc = 0.0\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    epoch_counter = 0\n",
    "\n",
    "    time1 = time.time()\n",
    "    \n",
    "    I_permutation = np.random.permutation(L_Y_train)\n",
    "\n",
    "    for i in range(0, L_Y_train, batch_size):\n",
    "\n",
    "        x_input = [x_train[j] for j in I_permutation[i:i+batch_size]]\n",
    "        y_input = np.asarray([y_train[j] for j in I_permutation[i:i+batch_size]],dtype=np.int)\n",
    "#         x_input = Variable(torch.FloatTensor(x_input)).cuda()\n",
    "#         target = Variable(torch.FloatTensor(y_input)).cuda()\n",
    "        target = torch.FloatTensor(y_input)\n",
    "        optimizer.zero_grad()\n",
    "        loss, pred = model(x_input,target)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()   # update weights\n",
    "        \n",
    "        prediction = pred >= 0.0\n",
    "        truth = target >= 0.5\n",
    "        acc = prediction.eq(truth).sum().cpu().data.numpy()\n",
    "        epoch_acc += acc\n",
    "        epoch_loss += loss.data[0]\n",
    "        epoch_counter += batch_size\n",
    "\n",
    "    epoch_acc /= epoch_counter\n",
    "    epoch_loss /= (epoch_counter/batch_size)\n",
    "\n",
    "    train_loss.append(epoch_loss)\n",
    "    train_accu.append(epoch_acc)\n",
    "\n",
    "    print(epoch, \"%.2f\" % (epoch_acc*100.0), \"%.4f\" % epoch_loss, \"%.4f\" % float(time.time()-time1))\n",
    "\n",
    "    # ## test\n",
    "    model.eval()\n",
    "\n",
    "    epoch_acc = 0.0\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    epoch_counter = 0\n",
    "\n",
    "    time1 = time.time()\n",
    "    \n",
    "    I_permutation = np.random.permutation(L_Y_test)\n",
    "\n",
    "    for i in range(0, L_Y_test, batch_size):\n",
    "\n",
    "        x_input = [x_test[j] for j in I_permutation[i:i+batch_size]]\n",
    "        y_input = np.asarray([y_test[j] for j in I_permutation[i:i+batch_size]],dtype=np.int)\n",
    "#         x_input = Variable(torch.FloatTensor(x_input)).cuda()\n",
    "#         target = Variable(torch.FloatTensor(y_input)).cuda()\n",
    "        target = torch.FloatTensor(y_input)\n",
    "        loss, pred = model(x_input,target)\n",
    "        \n",
    "        prediction = pred >= 0.0\n",
    "        truth = target >= 0.5\n",
    "        acc = prediction.eq(truth).sum().cpu().data.numpy()\n",
    "        epoch_acc += acc\n",
    "        epoch_loss += loss.data[0]\n",
    "        epoch_counter += batch_size\n",
    "\n",
    "    epoch_acc /= epoch_counter\n",
    "    epoch_loss /= (epoch_counter/batch_size)\n",
    "\n",
    "    test_accu.append(epoch_acc)\n",
    "\n",
    "    time2 = time.time()\n",
    "    time_elapsed = time2 - time1\n",
    "\n",
    "    print(\"  \", \"%.2f\" % (epoch_acc*100.0), \"%.4f\" % epoch_loss)\n",
    "\n",
    "torch.save(model,'BOW.model')\n",
    "data = [train_loss,train_accu,test_accu]\n",
    "data = np.asarray(data)\n",
    "np.save('data.npy',data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test accuracy typically seems to achieve its max after the 3rd epoch and begins to decrease with more training while the training accuracy continues to increase well into 90+%. This is a sure sign of overfitting. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
